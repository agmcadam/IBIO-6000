---
title: "Mixed-Effect Models in R"
author: "Andrew McAdam"
date: '2019-03-20'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\tableofcontents  
```{r packages, echo=F, message=FALSE}

```

# Note
This document is a work in progress.  In many places it is not as well annotated as I would like, but it's a start!

# Mixed-Effect Models
So far, we have talked about various types of statistical models, but all of the effects that we wanted to include as predictors of some response variable, were called 'fixed effects'.  That is, the influence of this variable is defined to be fixed, or constant.  In other cases, we might be interested in controlling for the effect of some predictor, but the effect of this predictor is not considered constant.  Instead, it is assumed to come from a distribution of possible effects.  We call these 'random effects'.  A mixed effect model is simply a model that contains both fixed and random effects as predictors.  

##  Difference Between Fixed and Random Effects
It can sometimes be difficult to understand the difference between fixed and random effects.  As a start, it is worth noting that while fixed effects can be categorical (e.g. effect of treatment A, B or C) or continuous (e.g. effect of temperature), random effects must always be categorical.  Recall that there is a philosophical difference between a fixed effect and a random effect.  When we include a categorical variable as a fixed effect we are saying that we care exactly what the effects are of each level in that factor.  For example, we care what the effect of the pesticide treatment is on crop yield relative to the control, or the application of nematodes to the fields.  That is, we care what those specific parameters are for each level of the factor.  For random effect, we don't care what the specific effects of each level of the factor are, we are just interested in controlling for variation in that factor.  For example, we might have applied each of these treatments in each of 25 farms accross southern Ontario.  We really don't care what the effect of farm #2 is relative to farm #22, we just want to control for the fact that farms are likely to differ in their yields and we want to account for this lack of independence in observations.  When we fit 'farm' as a random effect, we are not really estimating the effect of each farm, per se.  Instead what we are estimating is the *variance in yield among farms*.  

What we are saying by fitting farm as a random effect is that if we were to repeat the experiment, it would make perfect sense to use a totally different group of farms.  There was nothing special about the 25 that we used originally, we just needed replication in farms and wanted to control for the fact that we had repeated observations for each farm.  Sometimes people say that 'random effects' are used because the levels of the factor are selected 'at random' from all possible levels.  This is a decent description and is often true, but just because you didn't select your levels of the factor at random doesn't mean that you cannot fit the factor as a random effect.  For example, you might only have access to 10 greenhouses within which you can perform your experiments.  So there weren't selected at random.  Nevertheless you might not care about the specific greenhouses and want to control for differences in yoru response variable among the greenhouses, so you might fit *greenhouse* as a random effect.

## Number of Levels
Beyond the philosophical difference between fixed and random effects, there is also a practical distinction that you need to pay attention to. In order to get reasonable estimates for a random effect this effect needs to have at least 6 levels (e.g. 6 greenhouses, 6 fields, 6 populations, 6 individuals).  The reason for this is that, unlike fixed effects, mixed models estimate a variance for a random effect (i.e. variance among the levels of your random factor).  It starts to get very hard to measure a variance among fewer than 6 levels and this can cause big problems (i.e. unstable estimates) for random effects with less than 6 levels.  So if you have a factor that ought to be considered a random effect, but which only has 5 levels, then fit it as a fixed effect instead.

There are some additional packages that we will load for Mixed Models.
```{r}
library (lme4)

```


We are going to return to the red squirrel cone handling example where we were interested in the effects of cone handling behavior and cone morphology on the time taken to harvest the seeds from a cone.  Remember that in this example we had multiple cone handling observations per individual ("squirrel").  We will account for this sub-sampling here.

I am going to load this csv file here.
```{r}
ConeHandling<-read.table("data/ConeHandling.csv", sep=",", header=T)

summary (ConeHandling)
```
## Meta-Data
These data were from an experiment that I ran with field assistants as part of the Kluane Red Squirrel Project.  I was interested in whether the handedness of squirrels affected the speed with which they could extract seeds from spruce cones with cone scales arranged in a dextral or sinistral pattern.  Instead of measuring cone harvesting by squirrels, I instead asked field assistants to remove seeds one at a time, while spiraling the cone like a left-handed or right-handed squirrel. Each field assistant ('squirrel') was timed handling more than one cone. 


* *Squirrel* - name of the field assistant who handled the cone.

* *Trial* - trial number for that particular squirrel (1-6)

* *Length* - length of the cone (mm)

* *Handed* - whether the field assistant handled the cone in a right-handed (R) or left-handed (L) way.

* *Chirality* - whether the cone scales were arranged on the cone in a dextral (D) or sinistral (S) pattern.

* *Time* - time it took the assistant to remove all the seeds from the cone (seconds) **response variable**

* *Sex* - sex of the field assistant (M = male; F = female).  This should actually be called gender since I was unaware of the sex of each of the field assistants and instead inferred gender based on the way they presented themeslves.


# A Starting Example

Let's start with the simplest possible example.  Let's just say that we were interested in the average time that it takes to harvest seed from a cone.
```{r}
cone<-lm(Time~1, data=ConeHandling)
summary (cone)
```

So this model that contains only the intercept provides us with an estimate of the mean time taken to harvest a cone (along with a standard error and a test of whether this differs from zero).  But this inference is based on the fact that these 39 observations are independent, and we know that they are not.  We measured each squirrel more than once.  We can look at this graphically as.

```{r}
plot(ConeHandling$Squirrel, resid(cone))
```

This means that our inference will be overly liberal since we do not have independent observations (or more importantly errors).  Also because there are differences among squirrels our overall mean time is not a mean of the squirrels but a mean of all observations and will be affected by the number of observations that we have for each squirrel.

We can account for these differences among squirrels by including squirrel as a fixed effect in the model.

```{r}
cone2<-lm(Time~Squirrel, data=ConeHandling)
summary (cone2)
```

But this doesn't really do what we want.  We are not so much interested in estimating the mean length of time for each squirrel to eat their cones.  Instead we simply want a population mean value, but we want to account for the fact that we have multiple observations per squirrel and that squirrels vary in the length of time it takes them to eat a cone.  We can do this by instead including Squirrel identity as a random effect.

To do this we need to load a new package:
```{r}
library (nlme)
```

We can then fit a random effect using lme

```{r}
cone.lme<-lme(Time~1, random=~1|Squirrel, data=ConeHandling)
```
Note the notation for specifying a random effect compared to a fixed effect.

```{r}
summary(cone.lme)
```

Note that we have a slightly different output here.   Note that instead of getting estimates of the effect of each squirrel (fixed effects) we now have an estimate of the amount of variance among squirrels in the amount of time it takes for them to eat a cone (sd=22.4).  Note also that we had 39 observations for 9 squirrels.  Note also that our estimate of the mean has now been adjusted.  This new estimate now takes into account the fact that we have multiple observations per squirrel.

Note also that we can convert our sd's into variances and calculate the proportion of variance explained by the random effect.
```{r}
(22.37429^2)/(22.37429^2+36.96217^2)
```

So about 27% of the variation in cone handling time is accounted for by variation among squirrels. In this case this is the proportion of the total variance explained by the random effect because we have included only the intercept as a fixed effect in the model.  Otherwise this would be the proportion of variance not explained by the fixed effects that was explained by the random effect.

You can think of this proportion as the **repeatability** of cone handling time for squirrels.

This is just a simple example that I have used for the purpose of demonstration.  We haven't actually tested anything yet.  But let's say we wanted to test for an effect of Cone Handling behavior on handling time.  We could simply add the effect if Handed to our model.  This is analagous to a "Block" design where the effect of Handedness is blocked by Squirrel.  Here "Squirrel" is acting like "bench" or "field" or "plot" or "stand".

```{r}
cone.lme2<-lme(Time~Handed, random=~1|Squirrel, data=ConeHandling)
summary (cone.lme2)

#library (sjPlot)
#tab_model(cone.lme2)

```

Note that we have an estimate of the effect of Handedness on cone handling time that has taken into account the fact that we have multiple observations per squirrel

Note that now the residuals are independent of squirrels.

```{r}
plot(ConeHandling$Squirrel, resid(cone.lme))
```

## lme4
 We can now repeat this analysis using a different package called lem4. This package was written by the same person that wrote the nlme package, but it is more recent and can do many things that nlme cannot.
 
```{r}
library (lme4)
```
 
This might take a while.  If R tells you that a package is missing then download/load that and try and load lme4 again

or

require(lme4)

The notation for lme4 is slightly different from nlme.
```{r}
cone.lme3<-lmer(Time~Handed+(1|Squirrel), data=ConeHandling)
summary (cone.lme3)
```
So there are some important things to point out about this output.  First of all, note that we still have almost all of the same information.  Also note that the Std.Dev. is not a measure of the uncertainty of the random effect variance estimate but is instead simply the sqrt of the variance estimate.  I guess so people prefer one while others prefer the other so they have output both.

```{r}
sqrt(555.6)
```

## P-values for fixed effects in mixed-effect models
OK so many of you have probably noticed that for the fixed effects we don't have a df for the T test statistic and we don't have a p-value.  Yikes, no P-value!  What are we to do with no P-value?  **Now is when I warn you that I am going to tell you something that will disturb many of you for a couple of reasons.**  These important bits of information are not included because there is currently no consensus on a general way for these df to be calculated.  Without a specified df we cannot calculate a p-value.  In some special cases, the appropriate calculations are known, but there is not a general solution here.  So rather than provide estimates that are just that - estimates, the authors of the package decided not to report these values.  Note also that you will get df and p-valuess if you run the model with the older nlme and also with proc MIXED in SAS.  However, just because these functions provide this information doesn't mean that it is right!  I will not go into the details of why, but i will simply say that this is something that the hard core statisticians are currently working on right now.  Expect a solution to this at some point.  For more info on this see the "[R] lmer, p-values and all that.pdf" file or visit https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html.  You could also see http://www.maths.bath.ac.uk/~jjf23/ELM/mixchange.pdf for updates to the Faraway "Extending the Linear Model" book.

There are several approaches for dealing with this situation: 

### Approach #1
If we are comfortable making our own decision about what the appropriate degrees of freedom are then we could calculate an appropriate p-value... 

```{r}
2*pt(2.014, df=29, lower.tail=FALSE)
```

Note that the 2 up front is because a t-test of a parameter in a model is always two-tailed.

Note that this result is the same as what was provided by nlme.  The 29 df are based on n=39 minus one df for the intercept and one for the parameter for handedness and one for each of the 8 additional levels of the random effect.  This is based on the assumption that the random effect consumes 8 df. 

**IMPORTANT** If I were to do a poll in class of how many people would be comfortable reporting the p-value from the nlme cone.lme2 model and how many people would be comfortable reporting the p-value above from the lmer cone.lme3 model and 2*pt(2.014, df=29, lower.tail=FALSE), I would bet that almost all would report the former and almost none would report the latter.  This is because you inherently trust a canned package and inherently distrust your own ability to do statistics.  After checking to see that the p-values are exactly the same, take some time to reconsider both your trust in canned packages (SAS, SPSS, R, etc) and in yourself!

The above example with 29 df is based on a simplifying assumption that the predicted random effects are the true values, which is why each additional level (beyond the intercept) consumes a single df.  The important issue here is that the random effects are not known but are predicted from the model.  It is not entirely straight-forward to sort out how many df should be used for a random effect when designs are not balanced.  It could range anywhere from 1 to K-1, where K is the number of levels in the random effect.  If you wanted to assess the range of possible p-values you could use an error df based on 1 df for the random effects and a separate analysis with K-1 df for the random effects and use these results to get a sense of the range of possible p-values (a brute force approach).  
For example, in this case what if we used 39 observations - 1df for the intercept - 1 df for *handedness* - 1 df for the random effect = 36 df.  This assumes that the random effect uses only one df.
```{r}
2*pt(2.014, df=36, lower.tail=FALSE)
```
Notice that the consequences for the p-value are not huge, so maybe we shouldn't worry so much about this.

This approach still assumes that the F (or t) distribution is the appropriate distribution to test the fixed effect parameters against and this is not always clear. Basically the statisticians need to get busy and sort all this mess out for us ecologists!

### Approach #2
We can assess the significance of the fixed effects by using our good friend the likelihood ratio test!  We can test the significance of Handedness by comapring the model with Handedness to the simpler model with just the intercept.

```{r}
cone.lme3<-lmer(Time~Handed+(1|Squirrel), data=ConeHandling)
cone.lme4<-lmer(Time~1+(1|Squirrel), data=ConeHandling)
anova (cone.lme4, cone.lme3, test="Chi")
```

Note the comments above the output regarding the fact that the models were refitted using ML rather than REML.  When models differ in fixed effects we need to use ML rather than REML as our measure of the fit of the model to the data.

Note that the simpler model is listed before the more complex one and that the null hypothesis is that the simpler model is better.  In this case the result is significant which indicates that the more complicated model is significantly better than the simpler one.

We normally use restricted maximum likelihood here (REML) which is "restricted" to consider the likelihood after taking into account the fixed effects in the model.  When you change the fixed effects in the model the likelihood based on REML are no longer directly comparable.  But we can use good old ML instead.  Bottom line.... when you are using likelihoods to assess the fit of models that differ in their fixed effect structure then you need to use maximum likelihood (REML=F) rather than Restricted Maximum Likelihood (REML=T; the default in R).  In other cases REML does a better job of estimating variances and likelihoods so it is preferred when you are NOT comparing models or when the models differ only in their random effects.

```{r}
cone.lme3<-lmer(Time~Handed+(1|Squirrel), data=ConeHandling, REML=F)
cone.lme4<-lmer(Time~1+(1|Squirrel), data=ConeHandling, REML=F)
anova (cone.lme4, cone.lme3, test="Chi")
```

Note that this is the same as above.

So the likelihood ratio test suggests that the effect of handedness is in fact significant.

We can similarly assess the signficance of the random effect also using a likelihood ratio test.

```{r}
cone.lme3<-lmer(Time~Handed+(1|Squirrel), data=ConeHandling)
cone3<-lm(Time~Handed, data=ConeHandling)
```

Since there was no random effect in the second model we needed to use lm rather than lmer.  Unfortunately these models are of a different "class" so we can't use the anova function for our LRT.

```{r}
#anova (cone3, cone.lme3, test="Chi")

# Gives an error
```
So we will need to perform the LRT ourselves

```{r}
as.numeric(2*(logLik(cone.lme3)-logLik(cone3, REML=T)))
```
Note that we needed to specify REML=T for the lm because the default for this model is not REML it is ML and our lmer model was fit with REML. We need these to be the same to have a valid comparison of the likelihoods.

```{r}
as.numeric(2*(logLik(cone.lme3)-logLik(cone3, REML=F)))
```
**Notice the difference!!!!**

We can test this ourselves against a Chi-sq distribution
```{r}
pchisq(4.257733, 1, lower=F)
```

So the random effect is significant.  That is the model with the random effect is significantly better fit to the data than the model without the random effect.  Note that this LRT was tested against 1 df because we are only estimating the one parameter with the random effect (the among squirrel variance in handling time) - we are not fitting a separate parameter for each squirrel!

The final twist that I will likely not go into in class is that using a Chisq distribution for a LRT is only an approximation.  We can get a more accurate estimate of the p-value for the test by using a parametric bootsrap approach.  The Faraway "Extending the Linear Model" book also provides a good description of this.  I will simply provide the code below.  Please refer to the Faraway book for more info.

McAdam check this code...
Why all the singular fit errors?
```{r message=FALSE}
lrstat<-numeric(1000)

 for (i in 1:1000){
  rTime<-unlist(simulate(cone3))
  cone3r<-lm(rTime~Handed, data=ConeHandling)
  cone.lme3r<-lmer(rTime~Handed+(1|Squirrel), data=ConeHandling)
  lrstat[i]<-2*(logLik(cone.lme3r)-logLik(cone3r, REML=T))
  }
``` 
This vector "lrstat" is a vector of 1000 boostrapped LRT test statistics.  We can therefore determine what percentage of these are as extreme or more extreme than the 4.26 value that we calculated above.

```{r}
mean(lrstat>4.257733)
```

Note that this is a bootstrap and is based on only 1000 iterations so the number will not be the same each time.

This is similar to what we calculated before based on Chisq.

Now that we have gone to great lengths to use this likelihood ratio approach, I will warn you that likelihood ratio tests are not recommended for testing the fixed effects in mixed effect models because they are unreliable when sample sizes are moderate to low (see Bolker et al. 2009).  Bolker et al. 2009 recommend only using LRTs when the ratio of total sample size to # fixed effects are very large and the number of levels of the random effect are very large.

### Approach #3
I wanted to present you with some alternatives, but there is a package that has been written that will allow you to get df and p-values from fixed effects in mixed effect models.  The package is called *lmerTest*.
```{r}
library (lmerTest)
citation("lmerTest")
```

Once we have loaded the package into our R session, the package will provide p-values for the fixed effects when we use the summary command.  Nothing else needed!

```{r}
cone.lme3<-lmer(Time~Handed+(1|Squirrel), data=ConeHandling)
summary (cone.lme3)
```

Note that I didn't need to specify anything for lmerTest.  Just once it is loaded then a summary of an lmer model will automatically estimate p-values using a df based on what is called a Satterthwaite approximation.  Note that the error df for the fixed effect is estimated to be 29.04 here.  This is similar to what we calculated before.  I don't know exactly how the Sattherwaite approximation estimates these df, but this is the technique that is used by SAS in proc Mixed.  REMEMBER though that this is just ONE solution and there is no consensus among statisticians that this is an appropriate answer!!!  Just because SAS does it doesn't mean that it is necessarily correct!!!

For now, however, this appears to be the widely accepted approach for estimating df and p-values for fixed effects in a mixed-effect model.

# ANCOVA Example

I now want to leave this simple example and move to the ANCOVA example that we talked about in class.

```{r}
cone.lme5<-lme(Time~Width, random=~1|Squirrel, data=ConeHandling)
cone.lme6<-lmer(Time~Width+(1|Squirrel), data=ConeHandling)
summary (cone.lme6)
```

We could assess the significance of the random effect using the procedures described above.  

# Assessing Diagnostics with Mixed-Effect Models

We will work with our most recent model and assess the diagnostics for this model using both lmer and nlme

```{r}
cone.lme6<-lmer(Time~Width+(1|Squirrel), data=ConeHandling)
cone.lme5<-lme(Time~Width, random=~1|Squirrel, data=ConeHandling)
```

## Test for Heteroscedasticity

```{r}
plot(cone.lme5)
```

or

```{r}
plot(cone.lme6)
```

Are the within squirrel residuals centered on zero and have a constant variance?

```{r}
plot(cone.lme5, Squirrel~resid(.), abline=0)
```

The boxes are somewhat centered around zero.  The variances look like they differ among groups but this is hard to tell with so few observations in each group.  You should look to see if there is some consistent pattern in either the average residual or the size of the box between some known groups in the data.  For example, do males consistently have higher variance in residuals than females?

OR

```{r}
plot(resid(cone.lme6)~ConeHandling$Squirrel)
abline(h=0)
```

The overall fit of the model can be assessed using

```{r}
plot(cone.lme5, Time~fitted(.), id=0.05, adj=-0.3)
```

This plots the model fitted values against observed values.  Any point with an unusual value (id=0.05) is labeled and the label is offset from the data point (adj=-0.3). id must be a number between zero and 1.  Points with standardized residuals greater than (1-id)/2 in absolute value will be identified.

This can also be done (somewhat) using the more general
```{r}
plot(ConeHandling$Time~fitted(cone.lme6))
```

## Normality of Residuals

```{r}
qqnorm(cone.lme5, ~resid(.), id=0.05, adj=-0.75)
```

Note that the residuals and the expected values are reversed on the axes in that plot.  We can also use...

```{r}
qqnorm(resid(cone.lme6))
```

Note that the distribution of errors tends to have long tails.

## Assumptions about the random effects

We can predict the random effects from the model using
```{r}
ranef(cone.lme6)$Squirrel
```

Note that there is also a "ranef" function in the nlme package but when the lme4 package loaded it over-wrote it and so the ranef function for the lme4 package only runs with an lmer model.

```{r}
nlme::ranef(cone.lme5)
```

So we needed to use the "::" to specify that the 'ranef' function should be found in the nlme package

Remember that these random effects are not estimated by the model.  The model is used to PREDICT these values based on the ESTIMATED among squirrel variance.

```{r}
qqnorm (ranef(cone.lme6)$Squirrel[[1]])
```


So the distribution of these random effects looks good.

So the diagnsotics of this model look pretty good.


# Fitting Random Slopes
In this model we have fit a random effect for variation among squirrels in the average time it took them to harvest a cone.  However, we can also assess whether or not there is variation in their response to cones of various widths.  That is, does the slope of the relationship vary among squirrels.

```{r}
cone.lme7<-lme(Time~Width, random=~Width|Squirrel,  data=ConeHandling)
summary (cone.lme7)
```

Also 
```{r}
cone.lme8<-lmer(Time~Width+(Width|Squirrel), data=ConeHandling)
summary (cone.lme8)
```
Note that there is a very strong negative correlation between the random intercept and the random slope.  This can be caused by data that are not centered.  We can try and standardize the data first to see if this reduces this correlation but a correlation between the slope and intercept can also arise because there are few data points per individual.


We can ask whether this model that fits a random slope for each individual improves the fit of teh model to the data by comparing the fit of the two models.

```{r}
anova (cone.lme6, cone.lme8)
```

So the model with the random slope for each squirrel does not fit the data better than the model that assumed a constant slope.  So we prefer the simpler model where a common slope is assumed (cone.lme6).